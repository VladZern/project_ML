{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект: Идентификация пользователя по последовательности посещенных сайтов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'capstone_user_identification'\n",
    "data_train= pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'), index_col='session_id')\n",
    "data_test = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),  index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 21)\n",
      "(82797, 20)\n"
     ]
    }
   ],
   "source": [
    "print (data_train.shape)\n",
    "print (data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                time6    site7  \\\n",
       "session_id                       ...                                 \n",
       "1                           NaN  ...                  NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...  2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...  2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...  2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...  2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['time8', 'time9', 'time4', 'time5', 'time6', 'time7', 'time1', 'time2', 'time3', 'time10']\n"
     ]
    }
   ],
   "source": [
    "site_id = ['site1', 'site2', 'site3', 'site4', 'site5', 'site6', 'site7', 'site8', 'site9', 'site10']\n",
    "time_site = list(set(data_train.columns.values.tolist()) - set(site_id) - set(['target']))\n",
    "print(time_site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in time_site:\n",
    "    data_test[time] = pd.to_datetime(data_test[time])\n",
    "    data_train[time] = pd.to_datetime(data_train[time])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создадим новый признак, показывающий колличество открытых вкладок в каждой сессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['open_v']=10-np.sum(data_train[site_id].isnull(),axis=1)\n",
    "data_test['open_v']=10-np.sum(data_test[site_id].isnull(),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отсортируем данные по времени начала сессии (т.е время посещения первого сайта time1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.sort_values(by='time1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[site_id].fillna(0).to_csv('train_sessions_text.txt', sep=' ', index=None, header=None)\n",
    "data_test[site_id].fillna(0).to_csv('test_sessions_text.txt', sep=' ', index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразуем данные к виду: [session_id] [site_id1....site_idn...site_id1_2....site_idn_k]  т.е. подсчитаем для каждой сессии сколько раз был посещен тот или иной сайт и сколько раз группы сайтов посещались вместе (n-gramma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "with open('train_sessions_text.txt') as inp_train_file:\n",
    "    X_train_transform = vectorizer.fit_transform(inp_train_file)\n",
    "with open('test_sessions_text.txt') as inp_train_file:\n",
    "    X_test_transform = vectorizer.transform(inp_train_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 50000)\n",
      "(82797, 50000)\n"
     ]
    }
   ],
   "source": [
    "print (X_train_transform.shape)\n",
    "print (X_test_transform.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cоздадим следующие признаки:  Когда началась сессия: а) выходной или будний день, б) утром,днем,вечером или ночью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "def add_time_features(df, X_sparse):\n",
    "    hour = df['time1'].apply(lambda ts: ts.hour)\n",
    "    day_of_week = df['time1'].apply(lambda date: date.dayofweek)\n",
    "    weekend = (day_of_week>4).astype('int')\n",
    "    weekday = (day_of_week<5).astype('int')\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    X = hstack([X_sparse, morning.values.reshape(-1, 1), \n",
    "                day.values.reshape(-1, 1), evening.values.reshape(-1, 1), \n",
    "                night.values.reshape(-1, 1), weekend.values.reshape(-1, 1), weekday.values.reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = add_time_features(data_train.fillna(0), X_train_transform)\n",
    "X_test = add_time_features(data_test.fillna(0), X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_train['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В качетсве алгоритма (т.к. очень много признаков и матрицы разряженные) выберем линейную регрессию. Проверим алгоритм на TimeSeries кросс-валидации, т.к. сессии между собой также связаны по времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9107144155016706"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit #для данных с временной привязкой\n",
    "from sklearn.model_selection import cross_val_score\n",
    "model = LogisticRegression(C=1, random_state=17, solver='liblinear')\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "cv_scores = cross_val_score(model, X_train, y.astype('int'), cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Получили хорошу оценку на кросс-валидации, поэтому обучим модель и сделаем первые предсказания на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y)\n",
    "y_prob = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19759983e-05, 3.83841225e-08, 7.41228044e-08, ...,\n",
       "       1.24752215e-04, 5.86730149e-06, 1.30442299e-07])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file, target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels, index = np.arange(1, predicted_labels.shape[0] + 1), columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(y_prob[:,1], 'out_file_1.csv', target='target', index_label=\"session_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.197600e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.838412e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.412280e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.823858e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.281554e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id        target\n",
       "0           1  1.197600e-05\n",
       "1           2  3.838412e-08\n",
       "2           3  7.412280e-08\n",
       "3           4  7.823858e-09\n",
       "4           5  1.281554e-05"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = pd.read_csv('out_file_1.csv')\n",
    "out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попробуем улучшить качество модели путем подбора параметров регуляризации с помощью grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed: 13.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "       error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=17, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [0.01, 0.1, 1, 5], 'penalty': ['l1','l2']}\n",
    "model = LogisticRegression(random_state=17, solver='liblinear')\n",
    "grid_cv = GridSearchCV(model, param_grid, scoring='roc_auc', n_jobs=1, cv=cv, verbose=True)\n",
    "grid_cv.fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=17, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "{'penalty': 'l2', 'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print (grid_cv.best_estimator_) # лучший алгоритм\n",
    "print (grid_cv.best_params_) # лучший набор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_gs = grid_cv.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(y_prob_gs[:,1], 'out_file_2.csv', target='target', index_label=\"session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результирующая оценка  метрики aucroc согласно сайту KAGGLE составила: 0.94657 (что является хорошим показателем)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
